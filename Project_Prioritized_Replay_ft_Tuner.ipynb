{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuuZwide/Prioritized-Experience-Replay/blob/main/Project_Prioritized_Replay_ft_Tuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyQfsXvKHMJM",
        "outputId": "7cea8074-fb23-4718-d22c-3c2862b70323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting polygon-api-client\n",
            "  Downloading polygon_api_client-1.12.8-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: certifi<2024.0.0,>=2022.5.18 in /usr/local/lib/python3.10/dist-packages (from polygon-api-client) (2023.7.22)\n",
            "Collecting urllib3<2.0.0,>=1.26.9 (from polygon-api-client)\n",
            "  Downloading urllib3-1.26.17-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.3 (from polygon-api-client)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets, urllib3, polygon-api-client\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "Successfully installed polygon-api-client-1.12.8 urllib3-1.26.17 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install -U polygon-api-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kkEcdPouqFss"
      },
      "outputs": [],
      "source": [
        "!pip install keras-tuner -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z376_Lo3Fnjm",
        "outputId": "1305d836-cede-4822-801b-28f3e8eccb40"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-dca1ac0768ec>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "drive.mount('/content/drive')\n",
        "import random\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Model\n",
        "import math\n",
        "from datetime import datetime, timedelta\n",
        "from IPython.core.display import clear_output\n",
        "import polygon\n",
        "from polygon import RESTClient\n",
        "import matplotlib.pyplot as plt\n",
        "import keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LOtV-UYb_m42"
      },
      "outputs": [],
      "source": [
        "n = 7\n",
        "today = datetime.today()\n",
        "new_date = today - timedelta(days=n)\n",
        "formatted_date = new_date.strftime('%Y-%m-%d')\n",
        "print(formatted_date)\n",
        "\n",
        "sysdate = today.strftime('%Y-%m-%d')\n",
        "pdate = today - timedelta(days=n)\n",
        "pdate = pdate.strftime('%Y-%m-%d')\n",
        "\n",
        "print('Candlestict date' ,sysdate)\n",
        "print('indicators date',pdate )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X85BddJbHD9q"
      },
      "outputs": [],
      "source": [
        "forex_client = RESTClient('Y_MQfuWV7a5mqIVZeIiB4Y7P4Z9FrpRq')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k-WEF2D0HSSw"
      },
      "outputs": [],
      "source": [
        "mv_response = forex_client.get_sma(ticker = 'C:EURUSD',timespan = 'minute', window = 7, order = 'desc',timestamp_gt= '2023-08-21', limit =5000 )\n",
        "\n",
        "ema_response  = forex_client.get_ema(ticker = 'C:EURUSD',timespan = 'minute', window = 7, order = 'desc',timestamp_gt= '2023-08-21', limit =5000)\n",
        "\n",
        "macd_response = forex_client.get_macd(ticker = 'C:EURUSD',timespan = 'minute',short_window = 12, long_window = 26, signal_window = 9 ,\n",
        "                                      timestamp_gt= '2023-08-21', series_type = 'close', order = 'desc', limit = 5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pQ3BbK8THxyE"
      },
      "outputs": [],
      "source": [
        "def convertToDataFrame(mvIndater,valueName):\n",
        "    ma_list = []\n",
        "    for indicator_value in mvIndater.values:\n",
        "        ma_list.append({\n",
        "            'timestamp': indicator_value.timestamp,\n",
        "             'value': indicator_value.value,\n",
        "         })\n",
        "\n",
        "    ma_df = pd.DataFrame(ma_list)\n",
        "    #For debugging purposes\n",
        "    ma_df = ma_df.rename(columns={'value':valueName})\n",
        "    return ma_df\n",
        "\n",
        "\n",
        "def convertToDataFrameMACD(macdIndicator):\n",
        "    macd_list = []\n",
        "    for indicator_value in macdIndicator.values:\n",
        "        macd_list.append({\n",
        "            'timestamp': indicator_value.timestamp,\n",
        "            'value':indicator_value.value,\n",
        "            'signal':indicator_value.signal,\n",
        "            'histogram':indicator_value.histogram,\n",
        "        })\n",
        "\n",
        "    macd_df = pd.DataFrame(macd_list)\n",
        "    #macd = macd_df[['value','signal','histogram']]\n",
        "    return macd_df\n",
        "\n",
        "def generateCandleSticks(client, start_date, end_date, limit = 100 ):\n",
        "\n",
        "    response =  client.get_aggs(ticker = 'C:EURUSD',multiplier = 1,timespan = 'minute',\n",
        "                                  from_= start_date, to = end_date, sort = 'desc',limit = limit)\n",
        "\n",
        "    df = pd.DataFrame(response)\n",
        "    df['date'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "    final_df = df[[ 'open', 'high', 'low', 'close']]\n",
        "    final_df = final_df.rename(columns={'open':'Open', 'high':'High', 'low':'Low', 'close':'Close'})\n",
        "\n",
        "    return final_df,df\n",
        "\n",
        "\n",
        "def createChart(candleSticks, ma, ema, macd):\n",
        "\n",
        "    obj = [candleSticks, ma, ema, macd]\n",
        "    chart = pd.concat(obj, axis = 1)\n",
        "    chart = chart.dropna()\n",
        "    return chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Awq9y_tOK2kj"
      },
      "outputs": [],
      "source": [
        "candleStick, df = generateCandleSticks(forex_client, start_date = pdate, end_date = sysdate, limit = 5000 )\n",
        "#candleStick.head()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hXPnIlqqItK4"
      },
      "outputs": [],
      "source": [
        "sma = convertToDataFrame(mv_response,'SMA')\n",
        "sma['date'] = pd.to_datetime(sma['timestamp'], unit='ms')\n",
        "sma.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KZOk9u37JkQB"
      },
      "outputs": [],
      "source": [
        "ema = convertToDataFrame(mv_response,'EMA')\n",
        "ema['date'] = pd.to_datetime(ema['timestamp'], unit='ms')\n",
        "ema.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IN469Aw-Jsoc"
      },
      "outputs": [],
      "source": [
        "macd = convertToDataFrameMACD(macd_response)\n",
        "macd['date'] = pd.to_datetime(macd['timestamp'], unit='ms')\n",
        "macd.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qZCwFGPnKixi"
      },
      "outputs": [],
      "source": [
        "chart = createChart(candleSticks =candleStick , ma = sma['SMA'], ema = ema['EMA'], macd = macd[['value',\t'signal',\t'histogram']])\n",
        "chart.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eTzk-iB1Mi9k"
      },
      "outputs": [],
      "source": [
        "rnnChart = chart.values\n",
        "rows,cols = rnnChart.shape\n",
        "print(rows,cols)\n",
        "\n",
        "def convertToSequence(chart,timeSteps,closes):\n",
        "  rnnChart = chart.values\n",
        "  input_sequences = []\n",
        "  target_sequences = []\n",
        "\n",
        "  for i in range(len(rnnChart) - timeSteps):\n",
        "    input_sequences.append(rnnChart[i:i+timeSteps])\n",
        "    target_sequences.append(closes[i + timeSteps - 1])\n",
        "\n",
        "  input= np.array(input_sequences)\n",
        "  target= np.array(target_sequences)\n",
        "\n",
        "  return input, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3QTj-JRi5z5p"
      },
      "outputs": [],
      "source": [
        "state = np.random.random((4500, 5, 7))\n",
        "action = np.random.random((4500, 2))\n",
        "\n",
        "print(\"State shape:\", state.shape)\n",
        "print(\"Action shape:\", action.shape)\n",
        "\n",
        "# Expand dimensions of action to match the time steps dimension of state\n",
        "action_expanded = np.expand_dims(action, axis=1)\n",
        "action_expanded = np.tile(action_expanded, (1, 5, 1))  # Repeat along the time steps\n",
        "\n",
        "# Concatenate state and expanded action along the last axis (feature axis)\n",
        "combined_array = np.concatenate((state, action_expanded), axis=-1)\n",
        "\n",
        "print(\"Combined array shape:\", combined_array.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mgyFYRnV4Y7V"
      },
      "outputs": [],
      "source": [
        "class Portfolio():\n",
        "    def __init__(self, starting_amount):\n",
        "        self.port_value = starting_amount\n",
        "        self.prev_port_value = starting_amount\n",
        "        self.starting_amount = starting_amount\n",
        "        self.transactionCost = starting_amount * 0.003\n",
        "        self.relative_vector = None\n",
        "        self.prev_relative_vector = np.array([0, 0])\n",
        "        self.prev_weights = np.array([0, 0])\n",
        "        self.hold_weight = 0.0005\n",
        "        self.returns = []\n",
        "        self.counter = 0\n",
        "\n",
        "    def update_port(self, weights, prev_prices, cur_prices):\n",
        "        prev = np.hstack(([1], prev_prices))\n",
        "        curr = np.hstack(([1], cur_prices))\n",
        "        self.relative_vector = curr / prev\n",
        "\n",
        "        self.prev_weights = weights\n",
        "\n",
        "        return\n",
        "\n",
        "    def calculate_reward(self, weights):\n",
        "\n",
        "        rateReturn = np.dot(self.relative_vector, np.array((0,1)))\n",
        "\n",
        "        if(np.array_equal(weights, self.prev_weights) == False):\n",
        "           rateReturn -= 0.000015 #Transaction cost\n",
        "\n",
        "        rateReturn = rateReturn\n",
        "        if (weights[1] > 0.9):\n",
        "          rateReturn = rateReturn - 1.00002\n",
        "        else:\n",
        "          rateReturn = -0.00002\n",
        "\n",
        "        self.counter += 1\n",
        "        reward = rateReturn\n",
        "        self.prev_weights = weights\n",
        "        return reward\n",
        "\n",
        "\n",
        "    def clear_returns(self):\n",
        "        self.returns = []\n",
        "        self.counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3uAWI6fJ3-9w"
      },
      "outputs": [],
      "source": [
        "class Env():\n",
        "  def __init__(self, Portfolio, chart , closing_amounts, episode_length, last_index ): #add a model to my parameters\n",
        "    self.chart = chart\n",
        "    self.last_index = last_index\n",
        "    self.closing_amounts = closing_amounts\n",
        "    self.portfolio = Portfolio\n",
        "    self.og_episode_length = episode_length\n",
        "    self.episode_length = episode_length\n",
        "    self.index = 0\n",
        "    self.counter = 0 # Count the number of days the agent have been trading\n",
        "    self.done = False\n",
        "    self.trunc = False\n",
        "    self.starting_amount = Portfolio.starting_amount\n",
        "    self.trade_count = 0 # Number of times the agent has traded\n",
        "    self.prev_porfolio_value = self.portfolio.port_value\n",
        "    self.current_portfolio_value = self.portfolio.port_value\n",
        "    self.returns = 0\n",
        "\n",
        "  def reset(self): # Index is to deside when to start the trading period\n",
        "    self.episode_length = self.og_episode_length\n",
        "    self.portfolio = Portfolio(self.starting_amount)\n",
        "    self.index = random.randint(0, len(self.chart) -  self.last_index )\n",
        "    initial_weights = np.array([1, 0])\n",
        "    self.counter = 0\n",
        "    self.portfolio.prev_weights = [0,0]\n",
        "    self.portfolio.clear_returns()\n",
        "    self.state = self.add_gaussian_noise(self.chart[self.index])\n",
        "    return self.state\n",
        "\n",
        "  def validation_reset(self):\n",
        "    self.index = len(chart) - self.last_index #example value = 1000\n",
        "    initial_weights = np.array([1, 0])\n",
        "    self.episode_length = 900\n",
        "    self.counter = 0\n",
        "    self.portfolio.prev_weights = [0,0]\n",
        "    self.portfolio.clear_returns()\n",
        "    self.state = self.add_gaussian_noise(self.chart[self.index])\n",
        "    return self.state\n",
        "\n",
        "  def add_gaussian_noise(self, state, mean = 0.0, std_dev = 0.00001):\n",
        "    noise = np.random.normal(mean, std_dev, size=state.shape)\n",
        "    noisy_state = state + noise\n",
        "    return noisy_state\n",
        "\n",
        "  def check_done(self, portfolio_value):\n",
        "    done = False\n",
        "    threshhold =  portfolio_value/self.starting_amount\n",
        "    if threshhold > 0.01:\n",
        "      done = False\n",
        "    else:\n",
        "      done = True\n",
        "    return done\n",
        "\n",
        "  def check_trunc(self):\n",
        "    if(self.counter > self.episode_length):\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "  def step(self, action ):\n",
        "\n",
        "    if ((self.index + 1) < len(self.chart)):\n",
        "      prev_closing = self.closing_amounts[self.index]\n",
        "      cur_closing = self.closing_amounts[self.index + 1]\n",
        "      self.prev_prev_porfolio_value = self.current_portfolio_value\n",
        "      self.portfolio.update_port(action, prev_closing, cur_closing)\n",
        "      reward = self.portfolio.calculate_reward( action)\n",
        "      next_state = self.chart[self.index]\n",
        "      done = False\n",
        "      trunc = False\n",
        "    else:\n",
        "      next_state = self.chart[self.index]\n",
        "      done = True\n",
        "      reward = 0\n",
        "      trunc = True\n",
        "\n",
        "    if(self.check_done(self.current_portfolio_value)) is True:\n",
        "      done = True\n",
        "      next_state = self.chart[self.index]\n",
        "      reward = reward\n",
        "      trunc = False\n",
        "\n",
        "    if(self.check_trunc() is True):\n",
        "      trunc = True\n",
        "      done = False\n",
        "      reward = reward\n",
        "      next_state = self.chart[self.index]\n",
        "\n",
        "\n",
        "    self.counter = self.counter + 1\n",
        "    self.next_state =  self.add_gaussian_noise(next_state)\n",
        "    self.reward = reward\n",
        "    self.done = done\n",
        "    self.trunc = trunc\n",
        "    self.index = self.index + 1\n",
        "\n",
        "    return self.next_state, self.reward, self.done, self.trunc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oJzojPkeARxY"
      },
      "outputs": [],
      "source": [
        "class Preprocessor():\n",
        "  def __init__(self, timesteps, features):\n",
        "    self.normLayer = keras.layers.Normalization(axis = -1)\n",
        "\n",
        "  def adaptNorm(self,x):\n",
        "    self.normLayer.adapt(x)\n",
        "\n",
        "  def getPreprocessor(self):\n",
        "    return  self.normLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3Q9wXdA2AVPi"
      },
      "outputs": [],
      "source": [
        "class PreprocessingModel(tf.keras.Model):\n",
        "    def __init__(self, timesteps, features):\n",
        "        super(PreprocessingModel, self).__init__()\n",
        "\n",
        "        self.input_layer = keras.layers.InputLayer(input_shape=(timesteps, features))\n",
        "        self.normalisation_layer = keras.layers.Normalization(axis=-1)\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.input_layer(state)\n",
        "        out = self.normalisation_layer(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gcpIJtNz8Dmm"
      },
      "outputs": [],
      "source": [
        "class Actor():\n",
        "  def __init__(self, timesteps, features, preprocessor, learningRate):\n",
        "    self.timesteps = timesteps\n",
        "    self.features = features\n",
        "    self.preprocessor = preprocessor\n",
        "    self.learningRate = learningRate\n",
        "\n",
        "  def lossFunction(self, y_true, y_pred = None):\n",
        "    loss = -1 * tf.reduce_mean(y_true)\n",
        "    return loss\n",
        "\n",
        "  def buildModel(self):\n",
        "    model = keras.Sequential(name = \"Actor_Model\")\n",
        "    model.add(keras.layers.InputLayer(input_shape=(self.timesteps, self.features)))\n",
        "    model.add(self.preprocessor.getPreprocessor())\n",
        "    model.add(keras.layers.LSTM(units=256, activation='tanh', return_sequences=True))\n",
        "    model.add(keras.layers.LSTM(units=256, activation='tanh', return_sequences=False))\n",
        "    model.add(keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "    model.compile( optimizer = keras.optimizers.Adam(learning_rate = self.learningRate))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "stseCkQV6Dr5"
      },
      "outputs": [],
      "source": [
        "class Critic():\n",
        "  def __init__(self, timesteps, features, learningRate):\n",
        "    self.timesteps = timesteps\n",
        "    self.features = features\n",
        "    self.learningRate = learningRate\n",
        "\n",
        "  def buildModel(self):\n",
        "    model = keras.Sequential(name = \"Critic_Model\")\n",
        "    model.add(keras.layers.InputLayer(input_shape=(self.timesteps, self.features + 2)))\n",
        "    model.add(keras.layers.LSTM(units=256, activation='tanh', return_sequences=True) )\n",
        "    model.add(keras.layers.LSTM(units=256, activation='tanh', return_sequences=True))\n",
        "    model.add(keras.layers.LSTM(units=128, activation='tanh', return_sequences=True))\n",
        "    model.add(keras.layers.LSTM(units=64, activation='tanh', return_sequences=False))\n",
        "    model.add(keras.layers.Dense(1))\n",
        "\n",
        "    model.compile( optimizer = keras.optimizers.Adam(learning_rate = self.learningRate))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MVuuAe9a0Xn6"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer():\n",
        "  def __init__(self, max_size, timesteps,features, num_actions):\n",
        "    self.max_size = max_size\n",
        "    self.timesteps = timesteps\n",
        "    self.features = features\n",
        "    self.num_actions = num_actions\n",
        "    self.state_memory = np.zeros((self.max_size, timesteps, features))\n",
        "    self.action_memory = np.zeros((self.max_size, num_actions))\n",
        "    self.reward_memory = np.zeros(self.max_size)\n",
        "    self.next_state_memory = np.zeros((self.max_size,  timesteps,features))\n",
        "    self.terminal_memory= np.zeros(self.max_size, dtype = np.float32)\n",
        "    self.priorities = np.zeros(self.max_size, dtype = np.float32)\n",
        "    self.mem_counter = 1\n",
        "\n",
        "  def store_transitions(self,state, action, reward, done):\n",
        "    index = self.mem_counter % self.max_size\n",
        "    self.state_memory[index] = state\n",
        "    self.action_memory[index] = action\n",
        "    self.reward_memory[index] = reward\n",
        "    self.terminal_memory[index] = 1 - int(done) #False = 0 , True = 1 Cause if the state is done G will 0 cause the are no future rewards\n",
        "\n",
        "    if(max(self.priorities == 0)):\n",
        "      self.priorities[index] = 1\n",
        "    else:\n",
        "      self.priorities[index] = max(self.priorities)\n",
        "\n",
        "    if self.mem_counter > 0 :\n",
        "      self.next_state_memory[index - 1] = state\n",
        "    self.mem_counter += 1\n",
        "\n",
        "  def GetProbabilities(self, scale_a):\n",
        "    scaledProbs = self.priorities ** scale_a\n",
        "    sampleProbs = scaledProbs / sum(scaledProbs)\n",
        "    return sampleProbs\n",
        "\n",
        "  def GetImportance(self, probabilities):\n",
        "    importance = (1 / self.max_size) * (1 / probabilities)\n",
        "    normImportance = importance / max(importance)\n",
        "    return normImportance\n",
        "\n",
        "\n",
        "  def PrioritySample(self,batchSize,scale_a = 1):\n",
        "    mem_size = min(self.mem_counter, self.max_size)\n",
        "    sampleProbs = self.GetProbabilities(scale_a)\n",
        "    batch = random.choices(range(mem_size), k = batchSize , weights = sampleProbs[:mem_size])\n",
        "    importances = self.GetImportance(sampleProbs[batch])\n",
        "    states = self.state_memory[batch]\n",
        "    actions = self.action_memory[batch]\n",
        "    rewards = self.reward_memory[batch]\n",
        "    next_states = self.next_state_memory[batch]\n",
        "    dones = self.terminal_memory[batch]\n",
        "    indices = batch\n",
        "    return states, actions, rewards, next_states, dones, importances, indices\n",
        "\n",
        "  def SetPriorities(self, indices, errors, offset = 0.1):\n",
        "    for i, e in zip(indices, errors):\n",
        "      self.priorities[i] = e + offset\n",
        "\n",
        "  def random_sample(self, batch_size):\n",
        "    mem_size = min(self.mem_counter, self.max_size)\n",
        "    batch = np.random.choice(mem_size, batch_size)\n",
        "\n",
        "    states = self.state_memory[batch]\n",
        "    actions = self.action_memory[batch]\n",
        "    rewards = self.reward_memory[batch]\n",
        "    next_states = self.next_state_memory[batch]\n",
        "    dones = self.terminal_memory[batch]\n",
        "    return states, actions, rewards, next_states,dones\n",
        "\n",
        "\n",
        "  def clear_memory(self):\n",
        "    self.state_memory = np.zeros((self.max_size, self.input_shape))\n",
        "    self.action_memory = np.zeros((self.max_size, self.num_actions))\n",
        "    self.reward_memory = np.zeros(self.max_size)\n",
        "    self.next_state_memory = np.zeros((self.max_size, self.input_shape))\n",
        "    self.terminal_memory= np.zeros(self.max_size, dtype = np.float32)\n",
        "    self.mem_counter = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E6DfcKrXOvxI"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "  def __init__(self, memory , actor_model,critic_model , preprocessor,  batch_size, save_location ,gamma = 0,num_batches = 1):\n",
        "    self.num_batches = num_batches\n",
        "    self.features = features\n",
        "    self.timesteps = timesteps\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.Actor = actor_model\n",
        "    self.Critic = critic_model\n",
        "\n",
        "    self.memory = memory\n",
        "    self.learn_counter  = 0\n",
        "    self.preprocessor = preprocessor\n",
        "\n",
        "    self.save_location = save_location\n",
        "    self.act_loc = self.save_location + 'actor'\n",
        "    self.cri_loc = self.save_location + 'critic'\n",
        "\n",
        "    self.scale_b = 0.4\n",
        "    self.scale_a = 0.6\n",
        "\n",
        "\n",
        "  def choose_action(self, state):\n",
        "    weights = self.Actor(state)\n",
        "    action = weights.numpy()[0]\n",
        "    return action\n",
        "\n",
        "  def update_memory(self,state, action, reward, done):\n",
        "    self.memory.store_transitions(state, action , reward, done)\n",
        "\n",
        "  def save_models(self):\n",
        "    print('...saving models...')\n",
        "    self.Actor.save(self.act_loc)\n",
        "    self.Critic.save(self.cri_loc)\n",
        "\n",
        "  def load_models(self):\n",
        "    print('...load model....')\n",
        "    self.Actor = keras.models.load_model(self.act_loc)\n",
        "    self.Critic = keras.models.load_model(self.cri_loc)\n",
        "\n",
        "\n",
        "  def learn(self, step):\n",
        "\n",
        "    if (self.memory.mem_counter < self.batch_size ) and  (step % self.batch_size != 0)  :\n",
        "      return 0, 0\n",
        "    else:\n",
        "      self.learn_counter += 1\n",
        "      states, actions, rewards, next_states, dones,importanceWeights ,indices= self.memory.PrioritySample(self.batch_size)\n",
        "      actions = tf.cast(actions, dtype=tf.float32)\n",
        "\n",
        "      pstates = self.preprocessor(states)\n",
        "\n",
        "      criticActions = tf.expand_dims(actions, axis=1)\n",
        "      criticActions = tf.tile(criticActions, multiples=[1, self.timesteps, 1])\n",
        "      criticStates = tf.concat([pstates, criticActions], axis=-1)\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "\n",
        "        y = tf.cast(tf.expand_dims(rewards, axis=-1), dtype=tf.float32)\n",
        "        value_func = self.Critic(criticStates)\n",
        "        critic_loss = tf.losses.mean_squared_error(y, value_func)\n",
        "        error =  critic_loss\n",
        "        w = tf.cast(importanceWeights ** self.scale_b, dtype=tf.float32)\n",
        "        critic_loss = tf.multiply(error, w)\n",
        "        critic_loss = tf.reduce_mean(critic_loss)\n",
        "\n",
        "      critic_gradients = tape.gradient(critic_loss, self.Critic.trainable_variables)\n",
        "      self.Critic.optimizer.apply_gradients(zip(critic_gradients, self.Critic.trainable_variables))\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        current_actions = self.Actor(states)\n",
        "        current_actions = tf.expand_dims(current_actions, axis=1)\n",
        "        current_actions = tf.tile(current_actions, multiples=[1, self.timesteps, 1])\n",
        "        targetActorStates = tf.concat([pstates, current_actions], axis=-1)\n",
        "\n",
        "        actor_loss = self.Critic(targetActorStates)\n",
        "        actor_loss = -1 * tf.reduce_mean(actor_loss)\n",
        "\n",
        "      actor_gradients = tape.gradient(actor_loss, self.Actor.trainable_variables)\n",
        "      self.Actor.optimizer.apply_gradients(zip(actor_gradients, self.Actor.trainable_variables))\n",
        "\n",
        "      self.memory.SetPriorities(indices, error)\n",
        "      self.scale_b = self.scale_b * 1.001\n",
        "\n",
        "      self.learn_counter = 0\n",
        "    return actor_loss, critic_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BVULGHiUP3z5"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "max_size = 20000\n",
        "learning_rate = 0.0000125\n",
        "timesteps = 40\n",
        "critic_lr  =0.00005\n",
        "episode_length = 120\n",
        "starting_amount = 200\n",
        "batch_size = 64\n",
        "num_episodes = 500\n",
        "\n",
        "#Data\n",
        "Xchart, ychart = convertToSequence(chart,timesteps,chart['Close'])\n",
        "_, timesteps, features = Xchart.shape\n",
        "\n",
        "#Portfolio\n",
        "portfolio =  Portfolio(starting_amount = starting_amount)\n",
        "\n",
        "#Env\n",
        "env = Env(portfolio, Xchart, ychart , episode_length = episode_length, last_index = 1000)\n",
        "\n",
        "#Memory\n",
        "replayBuffer = ReplayBuffer( max_size = max_size, timesteps = timesteps ,features = features,num_actions = 2)\n",
        "\n",
        "#Preprocessor\n",
        "Critic_preprocessor = PreprocessingModel(timesteps = timesteps ,features = features)\n",
        "\n",
        "#Actor Preprocessor\n",
        "preprocessor = Preprocessor(timesteps = timesteps ,features = features)\n",
        "\n",
        "#Adapt the preprocessing layers\n",
        "Critic_preprocessor.normalisation_layer.adapt(Xchart)\n",
        "preprocessor.adaptNorm(Xchart)\n",
        "\n",
        "\n",
        "#Models\n",
        "actor = Actor(timesteps = timesteps, features = features, preprocessor = preprocessor, learningRate = learning_rate)\n",
        "critic = Critic(timesteps = timesteps, features =features , learningRate = critic_lr)\n",
        "critic_model = critic.buildModel()\n",
        "actor_model = actor.buildModel()\n",
        "actor_model.summary()\n",
        "critic_model.summary()\n",
        "\n",
        "#Saved location\n",
        "save_location = '/content/drive/MyDrive/Models/RNN Models/PRActorCritic/'\n",
        "\n",
        "#Agent\n",
        "agent = Agent( memory  = replayBuffer , actor_model = actor_model,critic_model = critic_model , preprocessor = Critic_preprocessor,\n",
        "              batch_size = batch_size, save_location = save_location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iitwn7DbqNxD"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MyHyperModel(keras_tuner.HyperModel):\n",
        "  def __init__(self, env, actor, number_of_episodes, Replay, preprocessor):\n",
        "    super(MyHyperModel).__init__()\n",
        "    self.env = env\n",
        "    self.actor = actor\n",
        "    self.num_episodes = number_of_episodes\n",
        "    self.memory = Replay\n",
        "    self.preprocessor = preprocessor\n",
        "    self.timesteps = 40\n",
        "    self.features = 9\n",
        "\n",
        "  def returnStates(self):\n",
        "    states, actions, rewards, _ , dones = self.memory.random_sample(batch_size)\n",
        "    actions = tf.cast(actions, dtype=tf.float32)\n",
        "    states = tf.cast(states, dtype=tf.float32)\n",
        "    rewards = tf.cast(rewards, dtype=tf.float32)\n",
        "    pstates = self.preprocessor(states)\n",
        "    criticActions = tf.expand_dims(actions, axis=1)\n",
        "    criticActions = tf.tile(criticActions, multiples=[1, self.timesteps, 1])\n",
        "    criticStates = tf.concat([pstates, criticActions], axis=-1)\n",
        "\n",
        "    return criticStates, states, rewards\n",
        "\n",
        "  def build(self,hp):\n",
        "    model = keras.Sequential(name = \"Critic_Model\")\n",
        "    model.add(keras.layers.InputLayer(input_shape=(self.timesteps, self.features + 2)))\n",
        "    model.add(keras.layers.LSTM(units=256, activation='tanh', return_sequences=True) )\n",
        "    model.add(keras.layers.LSTM(units=256, activation='tanh', return_sequences=True))\n",
        "    model.add(keras.layers.LSTM(units=128, activation='tanh', return_sequences=True))\n",
        "    model.add(keras.layers.LSTM(units=64, activation='tanh', return_sequences=False))\n",
        "    model.add(keras.layers.Dense(1))\n",
        "    return model\n",
        "\n",
        "  def fit(self, hp, model, x = None, y = None, validation_data = None, callbacks = None, **kwargs):\n",
        "\n",
        "    batch_size = 64\n",
        "    num_batches = 1\n",
        "    loss_metric = keras.metrics.Mean()\n",
        "    model.compile(optimizer = keras.optimizers.Adam(hp.Float(\"learning_rate\", max_value = 0.00003, min_value = 0.00001, sampling = \"log\")))\n",
        "\n",
        "    @tf.function\n",
        "    def run_train_steps(criticStates, states, rewards): #must take tensors as inputs\n",
        "      # for each batch open the gradient tape\n",
        "      with tf.GradientTape() as tape:\n",
        "        #run a forward pass\n",
        "        y = tf.cast(tf.expand_dims(rewards, axis=-1), dtype=tf.float32)\n",
        "        valueFunction = model(criticStates)\n",
        "        loss = tf.losses.mean_squared_error(y, valueFunction)\n",
        "        loss = tf.math.reduce_mean(loss)\n",
        "      #retrieve the gradients of the model\n",
        "      gradients = tape.gradient(loss, model.trainable_variables)\n",
        "      #the is when we apply gradient descent\n",
        "      model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        current_actions = self.actor(states, training=True)\n",
        "        current_actions = tf.expand_dims(current_actions, axis=1)\n",
        "        current_actions = tf.tile(current_actions, multiples=[1, self.timesteps, 1])\n",
        "\n",
        "        ActorStates = tf.concat([states, current_actions], axis=-1)\n",
        "        actor_loss = model(ActorStates,  training=True)\n",
        "        actor_loss = -1 * tf.math.reduce_mean(actor_loss)\n",
        "      gradients = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
        "      self.actor.optimizer.apply_gradients(zip(gradients, self.actor.trainable_variables))\n",
        "\n",
        "    @tf.function\n",
        "    def run_validation_step(criticStates, states, rewards):\n",
        "      y = tf.cast(tf.expand_dims(rewards, axis=-1), dtype=tf.float32)\n",
        "      valueFunction = model(criticStates)\n",
        "      loss = tf.losses.mean_squared_error(y, valueFunction)\n",
        "      loss = tf.math.reduce_mean(loss)\n",
        "      loss_metric.update_state(loss)\n",
        "\n",
        "\n",
        "    # Run through an episode to get the batches\n",
        "    step = 0\n",
        "\n",
        "    for epoch in range(2):\n",
        "      for e in range(self.num_episodes):\n",
        "        state = self.env.reset()\n",
        "        done, trunc = (False, False)\n",
        "        while not (trunc):\n",
        "          state = np.expand_dims(state, axis = 0)\n",
        "          action = agent.choose_action(state)\n",
        "          step += 1\n",
        "          next_state, reward, done, trunc = env.step(action)\n",
        "          self.memory.store_transitions(state, action , reward, done)\n",
        "          state = next_state\n",
        "\n",
        "          #Once we have memory big enough or mem_size mod 4 == 0 then we run a training step\n",
        "          if (self.memory.mem_counter > batch_size ) and (step % 4 == 0):\n",
        "\n",
        "            for i in range(num_batches):\n",
        "              criticStates, states, rewards = self.returnStates()\n",
        "              #Run Training Step\n",
        "              run_train_steps(criticStates, states, rewards)\n",
        "\n",
        "      for callback in callbacks:\n",
        "        callback.model = model\n",
        "\n",
        "      best_epoch_loss = float(\"inf\")\n",
        "\n",
        "      # Run validation step (Loss based validation or Reward based validation)\n",
        "      self.memory.clear_memory()\n",
        "      state = self.env.validation_reset()\n",
        "      done, trunc = (False, False)\n",
        "      while not (trunc):\n",
        "        state = np.expand_dims(state, axis = 0)\n",
        "        action = agent.choose_action(state)\n",
        "        step += 1\n",
        "        next_state, reward, done, trunc = env.step(action)\n",
        "        self.memory.store_transitions(state, action , reward, done)\n",
        "        state = next_state\n",
        "\n",
        "        #Now we run a validation pass and get the loss\n",
        "        if (self.memory.mem_counter > batch_size ) and (step % 4 == 0):\n",
        "          criticStates, states, rewards = self.returnStates()\n",
        "          run_validation_step(criticStates, states, rewards)\n",
        "\n",
        "      epoch_loss = float(loss_metric.result().numpy())\n",
        "      for callback in callbacks:\n",
        "        callback.on_epoch_end(epoch , logs = {\"my_metric\": epoch_loss}  )\n",
        "      loss_metric.reset_states()\n",
        "\n",
        "      print(f\"Epoch loss: {epoch_loss}\")\n",
        "      best_epoch_loss = min(best_epoch_loss, epoch_loss)\n",
        "\n",
        "    return best_epoch_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZNrk2lMxNfdm"
      },
      "outputs": [],
      "source": [
        "env = Env(portfolio, Xchart, ychart , episode_length = 100, last_index = 1000)\n",
        "\n",
        "model = MyHyperModel(env = env, actor = actor_model, number_of_episodes = 10, Replay = replayBuffer , preprocessor = Critic_preprocessor)\n",
        "\n",
        "\n",
        "tuner = keras_tuner.RandomSearch(\n",
        "    hypermodel= model,\n",
        "    objective= keras_tuner.Objective(\"my_metric\",\"min\"),\n",
        ")\n",
        "\n",
        "tuner.search()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zOyCJnjsTwar"
      },
      "outputs": [],
      "source": [
        "avg_scores = []\n",
        "avg_rewards = []\n",
        "avg_reward_history = []\n",
        "scores = 0\n",
        "rewards = 0\n",
        "score_history = []\n",
        "reward_history = []\n",
        "actor_loss_history = []\n",
        "critic_loss_history = []\n",
        "actor_avg_lesses = []\n",
        "critic_avg_lesses = []\n",
        "high_score = -1\n",
        "step = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-5yYICmJTxpp"
      },
      "outputs": [],
      "source": [
        "\n",
        "for e in range(num_episodes):\n",
        "    weights_history = []\n",
        "    highest = 0\n",
        "    state = env.reset()\n",
        "    rewards = 0\n",
        "    done, trunc = (False, False)\n",
        "    starting_index = env.index\n",
        "    print('episode: ', e)\n",
        "    while not (trunc):\n",
        "        state = np.expand_dims(state, axis = 0)\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, reward, done, trunc = env.step(action)\n",
        "        step += 1\n",
        "        rewards += reward\n",
        "        agent.update_memory(state, action, reward, done)\n",
        "        actor_loss,critic_loss  =  agent.learn(step)\n",
        "        state = next_state\n",
        "        weights_history.append(action)\n",
        "        if (env.current_portfolio_value > highest):\n",
        "          highest = env.current_portfolio_value\n",
        "\n",
        "    avg_reward_history.append(rewards)\n",
        "    avg_reward = np.mean(avg_reward_history[-10:])\n",
        "    avg_rewards.append(avg_reward)\n",
        "\n",
        "\n",
        "    if (avg_reward > high_score) & (e > 10) :\n",
        "      high_score = avg_reward\n",
        "      agent.save_models()\n",
        "\n",
        "\n",
        "    if e > 1 :\n",
        "      actor_loss_history.append(actor_loss)\n",
        "      avg_loss = np.mean(actor_loss_history)\n",
        "      actor_avg_lesses.append(avg_loss)\n",
        "\n",
        "      critic_loss_history.append(critic_loss)\n",
        "      avg_loss = np.mean(critic_loss_history)\n",
        "      critic_avg_lesses.append(avg_loss)\n",
        "\n",
        "    if  e > 10:\n",
        "        clear_output(wait=True)\n",
        "        fig, ax = plt.subplots(nrows=5, ncols=1, figsize=(7, 16))\n",
        "        ax[0].plot(avg_rewards)\n",
        "        ax[0].set_xlabel('Episodes')\n",
        "        ax[0].set_ylabel('Average Reward')\n",
        "\n",
        "        ax[1].plot(actor_avg_lesses)\n",
        "        ax[1].set_xlabel('Episodes')\n",
        "        ax[1].set_ylabel('Actor Avg Loss')\n",
        "\n",
        "        ax[2].plot(critic_avg_lesses)\n",
        "        ax[2].set_xlabel('Episodes')\n",
        "        ax[2].set_ylabel('Critic Avg Loss')\n",
        "\n",
        "        ax[3].plot(ychart[starting_index:env.index])\n",
        "        ax[3].set_xlabel('Steps')\n",
        "        ax[3].set_ylabel('Asset Value')\n",
        "\n",
        "        ax[4].plot(weights_history)\n",
        "        ax[4].set_xlabel('Steps')\n",
        "        ax[4].set_ylabel('Weight')\n",
        "        ax[4].legend(['Cash', 'Asset'])\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kCnkVY3b3-AV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bG4XHu2RXPew"
      },
      "outputs": [],
      "source": [
        "agent.save_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZXcu-PEBc7mG"
      },
      "outputs": [],
      "source": [
        "class Test_env()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H2MJoqH9dAfJ"
      },
      "outputs": [],
      "source": [
        "#practice run"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMzvHPGzn1kb9Gh1igckGjW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}